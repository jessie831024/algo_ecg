
# Week 3: 

## Session 1: Hyperparameter tuning 
- Cross validation - different ways to split the data
  - Hold out
  - K fold CV
  - Stratified K fold CV
  - LOOCV
  - LPOCV
  - Shuffle split CV
  - TS CV
  - Group K fold
- Search strategy:
  - Grid search
  - Random search 
  - [Succesive halving](https://scikit-learn.org/stable/modules/grid_search.html#successive-halving-user-guide) (SH): SH is like a tournament among candidate parameter combinations. SH is an iterative selection process where all candidates (the parameter combinations) are evaluated with a small amount of resources at the first iteration. Only some of these candidates are selected for the next iteration, which will be allocated more resources. For parameter tuning, the resource is typically the number of training samples, but it can also be an arbitrary numeric parameter such as n_estimators in a random forest.

### Implementation and execution
- There is an implementation of search using sklearn successive halving on random search in the notebook dir. 
- Given the different search configurations, I have moved part of the code and modularised it in such a way that you only need make changes to the config file and run the code as a command line tool. 
- Below allows you to pick a config, save the search result in a pickle file and write standard output to a text file for inspection later. 
```
cd ./algo_ecg
python hyperparameter_tuning.py <one of the choices specified in the hyper_config.py> >> <name>.sto 
```
## Session 2: [Deep learning](https://github.com/jessie831024/algo_ecg/blob/main/reading/LIS%20teaching%20March%202024%20(1).pdf) 
- Basics and history of development
- Feedforward neural network and backpropagation 
- Convolutional neural network 
- Recurrent neural network

## Session 3: AFib detection using PPG 

