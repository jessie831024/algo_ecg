
# Week 2: 

## Session 1: Logistic regression  

### Concepts: 
- [Basics](https://towardsdatascience.com/introduction-to-logistic-regression-66248243c148)
- Cost function
  - [Negative log likelohood](https://medium.com/@ashisharora2204/logistic-regression-maximum-likelihood-estimation-gradient-descent-a7962a452332)
  - [Derivative of](https://medium.com/analytics-vidhya/derivative-of-log-loss-function-for-logistic-regression-9b832f025c2d#:~:text=Since%20the%20hypothesis%20function%20for,function%20follows%20a%20certain%20pattern)
- [Regularisation](https://compgenomr.github.io/book/logistic-regression-and-regularization.html) L1, L2, Elastic Net
- In scikit-learn, the logistic regression implementation provides several [solvers](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression) to optimize the logistic regression objective function. The choice of solver depends on the size of the dataset and whether the problem is regularized or not.
- Stochastic gradient descent (SGD): In traditional gradient descent, the algorithm aims to minimize a cost or loss function by iteratively adjusting the model parameters in the direction opposite to the gradient of the cost function with respect to the parameters. In SGD, the key difference is that instead of using the entire dataset to compute the gradient at each iteration, only a single randomly selected data point (or a small batch of data points) is used. This introduces randomness into the optimization process, as the gradient is computed on a "stochastic" (random) subset of the data.
- [Implementation](https://atmamani.github.io/projects/ml/implementing-logistic-regression-in-python/)


## Session 2: Support vector machine 
- [Basics](https://towardsdatascience.com/machine-learning-iv-support-vector-machines-kaggle-dataset-with-svms-57d7c885652a):
  - Margin, Hard margin, Soft margin 
  - Support vectors 
  - Hyperplane 
  - Hinge loss function 
  - Kernel concepts
  - Bias variance tradeoff 
- Andrew Ng's Stanford lecture (Coursera & Youtube) & [lecture notes](https://cs229.stanford.edu/main_notes.pdf)
- sklearn [SVC](https://scikit-learn.org/stable/modules/svm.html#svm-kernels) (with linear kernel) vs. [LinearSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html) 
- Python [implementation 1](https://www.pycodemates.com/2022/10/implementing-SVM-from-scratch-in-python.html?utm_content=cmp-true)
- Python [implementation 2](https://colab.research.google.com/drive/1nw3Cxy00W1Jpkd1Kmv9hhux6Euz9omNl?usp=sharing#scrollTo=x7sKawRDPd4m)
- SVM with [kernel support](https://towardsdatascience.com/support-vector-machines-learning-data-science-step-by-step-f2a569d90f76)
- Open source [liblinear](https://github.com/cjlin1/liblinear)
## Session 3: Decision tree & random forest 
- Basic concepts
- D3: Ross Quinlan is credited within the development of ID3, which is shorthand for “Iterative Dichotomiser 3.” This algorithm leverages entropy and information gain as metrics to evaluate candidate splits. Some of Quinlan’s research on this algorithm from 1986 can be found [here](https://hunch.net/~coms-4771/quinlan.pdf)
- C4.5: This algorithm is considered a later iteration of ID3, which was also developed by Quinlan. It can use information gain or gain ratios to evaluate split points within the decision trees. 
- CART: The term, CART, is an abbreviation for “classification and regression trees” and was introduced by Leo Breiman. This algorithm typically utilizes Gini impurity to identify the ideal attribute to split on. Gini impurity measures how often a randomly chosen attribute is misclassified. When evaluating using Gini impurity, a lower value is more ideal. 
- Pen and paper example of [C4.5](https://medium.com/@sumit-kr-sharma/understanding-c4-5-decision-tree-algorithm-3bf0981faf4f)