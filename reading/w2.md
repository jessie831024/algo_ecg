
# Week 2: 

## Session 1: Logistic regression  

### Concepts: 
- [Basics](https://towardsdatascience.com/introduction-to-logistic-regression-66248243c148)
- Cost function
- - [Negative log likelohood](https://medium.com/@ashisharora2204/logistic-regression-maximum-likelihood-estimation-gradient-descent-a7962a452332)
  - [Derivative of](https://medium.com/analytics-vidhya/derivative-of-log-loss-function-for-logistic-regression-9b832f025c2d#:~:text=Since%20the%20hypothesis%20function%20for,function%20follows%20a%20certain%20pattern)
- [Regularisation](https://compgenomr.github.io/book/logistic-regression-and-regularization.html) L1, L2, Elastic Net
- In scikit-learn, the logistic regression implementation provides several [solvers](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression) to optimize the logistic regression objective function. The choice of solver depends on the size of the dataset and whether the problem is regularized or not.
- Stochastic gradient descent (SGD): In traditional gradient descent, the algorithm aims to minimize a cost or loss function by iteratively adjusting the model parameters in the direction opposite to the gradient of the cost function with respect to the parameters. In SGD, the key difference is that instead of using the entire dataset to compute the gradient at each iteration, only a single randomly selected data point (or a small batch of data points) is used. This introduces randomness into the optimization process, as the gradient is computed on a "stochastic" (random) subset of the data.

## Session 2: Support vector machine 


## Session 3: Decision tree & random forest 
