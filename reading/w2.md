
# Week 2: 

## Session 1: Logistic regression  

### Concepts: 
- [Basics](https://towardsdatascience.com/introduction-to-logistic-regression-66248243c148)
- Cost function
  - [Negative log likelohood](https://medium.com/@ashisharora2204/logistic-regression-maximum-likelihood-estimation-gradient-descent-a7962a452332)
  - [Derivative of](https://medium.com/analytics-vidhya/derivative-of-log-loss-function-for-logistic-regression-9b832f025c2d#:~:text=Since%20the%20hypothesis%20function%20for,function%20follows%20a%20certain%20pattern)
- [Regularisation](https://compgenomr.github.io/book/logistic-regression-and-regularization.html) L1, L2, Elastic Net
- In scikit-learn, the logistic regression implementation provides several [solvers](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression) to optimize the logistic regression objective function. The choice of solver depends on the size of the dataset and whether the problem is regularized or not.
- Stochastic gradient descent (SGD): In traditional gradient descent, the algorithm aims to minimize a cost or loss function by iteratively adjusting the model parameters in the direction opposite to the gradient of the cost function with respect to the parameters. In SGD, the key difference is that instead of using the entire dataset to compute the gradient at each iteration, only a single randomly selected data point (or a small batch of data points) is used. This introduces randomness into the optimization process, as the gradient is computed on a "stochastic" (random) subset of the data.
- [Implementation](https://atmamani.github.io/projects/ml/implementing-logistic-regression-in-python/)


## Session 2: Support vector machine 
- [Basics](https://towardsdatascience.com/machine-learning-iv-support-vector-machines-kaggle-dataset-with-svms-57d7c885652a):
  - Margin, Hard margin, Soft margin 
  - Support vectors 
  - Hyperplane 
  - Hinge loss function 
  - Kernel concepts
  - Bias variance tradeoff 
- Andrew Ng's Stanford lecture (Coursera & Youtube) & [lecture notes](https://cs229.stanford.edu/main_notes.pdf)
- sklearn [SVC](https://scikit-learn.org/stable/modules/svm.html#svm-kernels) (with linear kernel) vs. [LinearSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html) 
- Python [implementation 1](https://www.pycodemates.com/2022/10/implementing-SVM-from-scratch-in-python.html?utm_content=cmp-true)
- Python [implementation 2](https://colab.research.google.com/drive/1nw3Cxy00W1Jpkd1Kmv9hhux6Euz9omNl?usp=sharing#scrollTo=x7sKawRDPd4m)
- SVM with [kernel support](https://towardsdatascience.com/support-vector-machines-learning-data-science-step-by-step-f2a569d90f76)
- Open source [liblinear](https://github.com/cjlin1/liblinear)
## Session 3: Decision tree & random forest 
