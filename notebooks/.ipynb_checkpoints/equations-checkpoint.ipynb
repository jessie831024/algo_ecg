{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c33a6fb-3084-4a18-ba5a-f431e3572713",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "https://compgenomr.github.io/book/logistic-regression-and-regularization.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8ce32f-dd46-420a-9447-627c0189af3c",
   "metadata": {},
   "source": [
    "Sigmoid function \n",
    "$$f(x) = \\frac{1}{1 + e^{-(x)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795dbe95-708c-45d5-b24f-2c83fbf20954",
   "metadata": {},
   "source": [
    "# Maximum Likelihood\n",
    "https://medium.com/@ashisharora2204/logistic-regression-maximum-likelihood-estimation-gradient-descent-a7962a452332"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc1316e-9ccd-4a4c-850e-98044579f342",
   "metadata": {},
   "source": [
    "The likelihood function quantifies the probability of observing the given data under a specific statistical model and given parameters. Likelihood function is defined as the product of the individual probalities of each data point given the parameters. \n",
    "\n",
    "$$L(\\beta) = \\prod p(x_{i})^{y_{i}} \\times (1-p(x_{i})^{1-y_{i}} )$$ \n",
    "\n",
    "where\n",
    "\n",
    "$x_{i}$ is the data point\n",
    "\n",
    "$y_{i}$ is the class being 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d353d4b-4c4d-419e-b8b5-2baac2997975",
   "metadata": {},
   "source": [
    "Considering the values of probability is very dense that lies between 0 and 1 doing a product would not be mathematical easier and feasible to do. However, taking the logarithm allows transforming the product of probabilities in the likelihood function into a sum of logarithms, which is generally easier to work with and we called this Log Likelihood.\n",
    "\n",
    "Log Likelihood function \n",
    "$$l(\\beta) = \\sum \\limits _{i=1} ^{n} y_{i}log(p(x_{i})) + (1-y_{i})log(1-p(x_{i}))$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a37148c-65cd-478b-bc65-b7b596084f89",
   "metadata": {},
   "source": [
    "Taking the logarithmic of probabilities value which lies between 0 and 1 will always return negative values. So, in order to have positive values we multiply it with -1 and by doing so we get the negative log-likelihood (which is the cost function). Under this, we try to minimize this negative log-likelihood and in return expects the best parameter values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1112153d-586a-4719-8e02-f619302cc307",
   "metadata": {},
   "source": [
    "# Cost function / cross-entropy function\n",
    "Cost function is the optimisation objective.  \n",
    "https://towardsdatascience.com/introduction-to-logistic-regression-66248243c148"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bef6c58-5d96-42f8-a1c4-799f025845d2",
   "metadata": {},
   "source": [
    "Hypothesis function\n",
    "$$h\\theta(X) = \\frac{1}{1+e^{-(\\beta_{0} + \\beta_{1}X)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cdc975-5d04-458a-bf97-9717f2b3c7dc",
   "metadata": {},
   "source": [
    "Cost function for binary classification\n",
    "\n",
    "For y = 1; \n",
    "$$cost(h_{\\theta}(x_{i}),y_{i}) = -log(h_{\\theta}(x_{i}))))$$\n",
    "For y = 0 \n",
    "$$cost(h_{\\theta}(x_{i}),y_{i}) = -log(1- h_{\\theta}(x_{i}))))$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94199ab6-927c-4a0a-aee2-384e9030a29e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e33aa13-08c4-4001-8a8f-53d3690f5bf3",
   "metadata": {},
   "source": [
    "The above two functions can be combined into a single function\n",
    "$$J(\\theta) = -\\frac{1}{m}\\sum[y^{(i)}log(h\\theta(x^{(i)})) + (1-y^{(i)})log(1-h\\theta(x^{(i)}))]$$\n",
    "\n",
    "where \n",
    "\n",
    "m: the number of observations\n",
    "\n",
    "x: features\n",
    "\n",
    "$\\theta$: parameters to be estimated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042d8798-141d-4cf6-8e4d-ef1ab44d3aea",
   "metadata": {},
   "source": [
    "# Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a27eb65-446c-4b0c-b3c4-522e0cafd64b",
   "metadata": {},
   "source": [
    "The goal of gradient descent is the mininise the cost value, min J($\\theta$) by solving the differential equation above \n",
    "$$\\frac{\\partial}{\\partial\\theta}J(\\theta) = \\frac{1}{m}\\sum(h_{\\theta}(x^{(i)}) - y^{(i)})x^{(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182d0e64-67c6-4243-bbd7-32031ebafbbe",
   "metadata": {},
   "source": [
    "How to get the derivative of cost function for logistic regression: \n",
    "https://medium.com/analytics-vidhya/derivative-of-log-loss-function-for-logistic-regression-9b832f025c2d#:~:text=Since%20the%20hypothesis%20function%20for,function%20follows%20a%20certain%20pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1874c121-96cc-49c4-aebd-277044fe6e46",
   "metadata": {},
   "source": [
    "To minimise out cost function we need to run the gradient descent function on each parameter: \n",
    "$$\\theta_{j} := \\theta_{j} - \\alpha\\frac{\\partial}{\\partial\\theta_{j}}J(\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216c1efd-d6f0-4357-9c43-ce25d6ab924a",
   "metadata": {},
   "source": [
    "$$\\theta_{j} := \\theta_{j} - \\alpha\\sum \\limits_{i=1}^{m}(h_{\\theta}(x^{(i)}) - y^{(i)})x^{(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ad0631-d9ec-4818-9151-8819aed40513",
   "metadata": {},
   "source": [
    "# Regularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0307110-42b1-4e88-b351-8a73fc82fe1e",
   "metadata": {},
   "source": [
    "The purpose of regularisation is to overcome the overfitting issue. There are different regularisation techniques by modifying the loss function with a penalty term which effectively shrinks the estimates of the coefficients. Therefore these types of methods within the framework of regression are also called “shrinkage” methods or “penalized regression” methods. L1 and L2 are by adding the following term to the loss function to penalise the coefficients (not the bias) which are also called \"Lasso\" and \"Ridge\" regression respecively. L2 is calculated as the square root of the sum of the squared vector values whereas L1 uses the absolute values instead of squared values. This term will help shrink the coefficients in the regression towards zero. Elastic net combines L1 and L2 with an additional weight parameter. \n",
    "\n",
    "L2 Ridge (Hoerl and Kennard 1970)\n",
    "$$J(\\theta) = -\\frac{1}{m}\\sum[y^{(i)}log(h\\theta(x(i))) + (1-y^{(i)}log(1-h\\theta(x(i)))) + \\frac{\\lambda}{m}\\sum \\limits _{j=1} ^{D}\\theta_{j}^{2}$$  \n",
    "\n",
    "L1 Lasso (Tibshirani 1996)\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{m}\\sum[y^{(i)}log(h\\theta(x(i))) + (1-y^{(i)}log(1-h\\theta(x(i)))) + \\frac{\\lambda}{m}\\sum \\limits _{j=1} ^{D}|\\theta_{j}|$$  \n",
    "\n",
    "Elastic Net(Zou and Hastie 2005) \n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{m}\\sum[y^{(i)}log(h\\theta(x(i))) + (1-y^{(i)}log(1-h\\theta(x(i)))) + \\frac{\\lambda}{m}\\sum \\limits _{j=1} ^{D}(\\alpha\\theta^2 + (1-\\alpha)|\\theta_{j}|$$  \n",
    "\n",
    "\n",
    "where \n",
    "\n",
    "D is the dimension of the features\n",
    "\n",
    "$\\lambda$ is the strength of the regularisation\n",
    "\n",
    "j is the number of parameters in the model\n",
    "\n",
    "$\\alpha$ controls the weight given to L1 and L2 penalty and it's between 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e777f4-d9e2-4a16-b4e8-f9eded163c93",
   "metadata": {},
   "source": [
    "# Local minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cf4c20-4e77-4d04-b45c-0c7621894c0f",
   "metadata": {},
   "source": [
    "Convexity of a function requires that the function has a unique global minimum and that any local minimum is also a global minimum. The negative log-likelihood function in logistic regression is not guaranteed to be convex, so it can have multiple local minima. For parameter optimisation iterative optimization algorithms like gradient descent is used but it's not guaranteed that the found solution is a glocal minimum. In more complex models, such as deep neural networks with multiple layers and non-linear activations, the NLL cost function can become non-convex. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bc65a2-9560-4da2-b058-fed52be28704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c7045a-4975-4ac9-ad57-b3eab91851f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cinc",
   "language": "python",
   "name": "cinc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
